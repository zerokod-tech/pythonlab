# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tnXGR2KLrfXlKZlq_aj9U9NBBW4KVGhK
"""

#source : https://github.com/ifrankandrade/model-building
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn import linear_model



# plot settings
sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks
plt.rc('axes', titlesize=23)     # fontsize of the axes title
plt.rc('axes', labelsize=20)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=16)    # fontsize of the tick labels
plt.rc('ytick', labelsize=16)    # fontsize of the tick labels
plt.rc('legend', fontsize=16)    # legend fontsize
plt.rc('font', size=16)          # controls default text sizes

"""# The Data"""

df_boston = pd.read_csv('Boston House Prices.csv')
df_boston

df_boston.describe()

"""# Linear Regression with Statsmodels

## Simple Linear Regression

### Define the dependent and independent variable
"""

y = df_boston['Value'] # dependent variable
x = df_boston['Rooms'] # independent variable

"""### Exploring the dataset"""

plt.figure(figsize=(12, 8), tight_layout=True)
sns.scatterplot(x=x, y=y)
plt.show()

"""### Making the regression: Adding a constant and fitting the model

Statmodels doesn't add a constant by default, so we have to do it on our own
"""

x = sm.add_constant(x) # adding a constant
lm = sm.OLS(y,x).fit() # fitting the model

lm.predict(x)

"""### The Regression Table"""

lm.summary()

"""OLS (Ordinary Least Squares): Most common method to estimate the linear regression (It finds the line that minimizes the sum of square error (SSE))

Table #1
- Dep. Variable: This is the dependent variable (in our example "Value" is our target value)
- R-squared: 1-(SSR/SST)
    1. Takes values from 0 to 1. R-squared values close to 0 correspond to a regression that explains none of the variability of the data, while values close to 1 correspond to a regression that explains the entire variability of the data. The r-squared obtained is telling us that the number of rooms explains 48.4% of the variability in house values.

Table #2

- Coef: Std error: Represents the accuracy of the prediction. The lower the standard error, the better prediction.
- Std error: The t scores and p-values are used for hypothesis test. The "Rooms" variable has a statistically significant p-value. Also, we can say at a 95% percent confidence level that the value of "Rooms" is between 8.279 to 9.925.

### Linear Regression Equation
"""

# Rooms coef: 9.1021
# Constant coef: - 34.6706

# Linear equation: ùë¶ = ùëéùë• + ùëè
y_pred = 9.1021*x['Rooms'] - 34.6706

"""### Plotting The Regression Line"""

# plotting the data points
plt.figure(figsize=(12, 8), tight_layout=True)
sns.scatterplot(x=x['Rooms'], y=y)

#plotting the line
sns.lineplot(x=x['Rooms'],y=y_pred, color='red')

#axes
plt.xlim(0)
plt.ylim(0)
plt.savefig('linear_regression')
plt.show()

"""## Multiple linear regression

### Define the dependent and independent variable
"""

y = df_boston['Value'] # dependent variable
X = df_boston[['Rooms', 'Distance']] # independent variable

"""### Making the regression: Adding a constant and fitting the model"""

X = sm.add_constant(X) # adding a constant
lm = sm.OLS(y, X).fit() # fitting the model
lm.summary()

"""# Linear Regression with Scikit-learn

## Define the dependent and independent(s) variable
"""

y = df_boston['Value'] # dependent variable
X2 = df_boston[['Rooms', 'Distance']] # independent variable
X1 = df_boston[['Rooms']] # independent variable



# plotting the data points
plt.figure(figsize=(12, 8), tight_layout=True)
sns.scatterplot(x='Rooms', y='Value', data=df_boston)

# plotting the data points
plt.figure(figsize=(12, 8), tight_layout=True)
sns.scatterplot(x='Distance', y='Value', data=df_boston)

"""## Fitting the Model"""

lm2 = linear_model.LinearRegression() # multiple linear regression
lm1 = linear_model.LinearRegression() # simple linear regression

lm2.fit(X2, y) # fitting the model
lm1.fit(X1, y) # fitting the model

X2.shape

"""## Predicting values"""

lm2.predict(X2)[:5]

y[:5]

lm1.predict(X1)[:5]

y[:5]

"""## Regression Table"""

# r2 score
lm2.score(X2, y)

lm2.coef_

lm1.coef_

lm2.intercept_

# plotting the data points
plt.figure(figsize=(12, 8), tight_layout=True)
sns.scatterplot(x='Rooms', y='Value', data=df_boston)

# predictions
y_pred = lm1.predict(X1)

#plotting the line
sns.lineplot(x='Rooms', y=y_pred, data=df_boston, color='red')

#axes
plt.xlim(0)
plt.ylim(0)
plt.savefig('linear_regression')
plt.show()

X1.shape

